# -*- coding: utf-8 -*-
"""a1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0i6QoGum-KamweQREuzcRDdxbod0M-B
"""

# Commented out IPython magic to ensure Python compatibility.
# read file from rt-polaritydata folder
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import string
import re
import collections
import time

import nltk
#nltk.download('stopwords')
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
#nltk.download('wordnet')
#nltk.download('rslp')

from nltk.corpus import stopwords
from nltk.tag import pos_tag
from nltk.stem import LancasterStemmer
from nltk.stem import RSLPStemmer
from nltk import word_tokenize
from nltk import ngrams
from nltk.tokenize import RegexpTokenizer
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import sent_tokenize
from nltk import wordpunct_tokenize

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report,confusion_matrix
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import ComplementNB
from sklearn import metrics
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier

"""Read data from files"""

TEST = True

#load data from rt-polaritydata folder
#devide data into trainset and testset with 9:1 split
if TEST:
    with open('rt-polaritydata//rt-polarity.pos', 'r', encoding='latin-1') as fingood, open('rt-polaritydata//rt-polarity.neg', 'r', encoding='latin-1') as finbad:
      reviews = ([(review, 1) for review in fingood.readlines()] + [(review, 0) for review in finbad.readlines()])

    dataset = np.asarray(reviews)
    X_train, X_test, y_train, y_test = train_test_split(dataset[:, 0], dataset[:, 1], test_size=0.1, random_state=42)
    np.save("X_train", X_train)
    np.save("X_test", X_test)
    np.save("y_train", y_train)
    np.save("y_test", y_test)
else:
    X_train = np.load("X_train.npy")
    X_test = np.load("X_test.npy")
    y_train = np.load("y_train.npy")
    y_test = np.load("y_test.npy")

"""**Preprocessing and feature extraction**"""
#remove stop words, most_common words and the rare words
def getStop(corpus, stops, common_nb=30):
    freq = nltk.FreqDist(corpus)
    # remove the most_common words and the rare words
    most_common = freq.most_common(common_nb)
    rare_words = freq.hapaxes()

    # remove rare words
    #wordCount = collections.Counter(corpus)  
    #sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)

    #rare_words = [item[0] for item in sortWordCount if item[1] >= 3]
    stops+=rare_words
    stops+=[word[0] for word in most_common]
    return stops

stops = getStop(np.append(X_train, X_test), nltk.corpus.stopwords.words('english'), 5)

# 3 types of tokenizers
# lemma tokenizer
class LemmaTokenizer:
    def __init__(self):
        self.wnl = WordNetLemmatizer()
    def __call__(self, doc):
        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in stops]

#stemmer tokenizer
class StemmerTokenizer:
    def __init__(self):
        self.stemmer = RSLPStemmer()
    def __call__(self, doc):
        return [self.stemmer.stem(t) for t in word_tokenize(doc) if t not in stops]

# will tokennize over all nonalphabet chars
tokenizer = RegexpTokenizer(r'[A-Za-z]\w+').tokenize

# method train, test, and draw confusion matrix
categories = ['neg', 'pos']
def train_test(pipeline, parameters, methodname, X_train, y_train, X_test, y_test):       
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv = 5,scoring='accuracy')

    print("Performing grid search...")

    t0 =time.time()
    grid_search.fit(X_train, y_train)
    print("done in %0.3fs" % (time.time() - t0))
    print()

    print("Best score: %0.3f" % grid_search.best_score_)
    print("Best parameters set:")
    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))
    
    estimator = grid_search.best_estimator_

    print('Test Accuracy: %.3f' % estimator.score(X_test, y_test))
    y_pred_test = estimator.predict(X_test)
    print(metrics.classification_report(y_test, y_pred_test))
    cm_test = confusion_matrix(y_test,y_pred_test )
    print(cm_test)

    ax = plt.axes()
    sns.heatmap(cm_test,annot=True,fmt='d',xticklabels=categories, yticklabels=categories,ax = ax)

    ax.set_title(methodname)
    plt.show()

# #############################################################################
# Define a pipeline combining a text feature extractor with a 
# LogisticRegression classifier
pipeline = Pipeline([
    ('vect', CountVectorizer(ngram_range= (1,1))),
    ('tfidf', TfidfTransformer()),
    ('lr', LogisticRegression(random_state=0))
])


parameters = {
    'vect__max_df': [0.5, 0.75, 1.0],
    'vect__tokenizer':[LemmaTokenizer(),StemmerTokenizer(), tokenizer],
    'vect__stop_words': [stops,None],
    'tfidf__use_idf': [True, False],
    'lr__penalty': ['l2', None],
    'lr__C': [0.1, 1.0, 10.0]
}

#%%time
train_test(pipeline, parameters, 'LogisticRegression Confusion Matrix', X_train, y_train, X_test, y_test)


# #############################################################################
# Define a pipeline combining a text feature extractor with a 
# SVM with linear kernel Classifier
pipeline = Pipeline([
    ('vect', CountVectorizer(ngram_range= (1,1))),
    ('tfidf', TfidfTransformer()),
    ('svm', SGDClassifier(random_state=42,max_iter=100,learning_rate='optimal'))
])

parameters = {
    'vect__max_df': [0.5, 0.75, 1.0],
    'vect__tokenizer':[LemmaTokenizer(),StemmerTokenizer(), tokenizer],
    'vect__stop_words': [None, stops],
    'tfidf__use_idf': [True, False],
    'svm__loss':['squared_hinge', 'hinge'],
    'svm__penalty':['l2', 'l1'],
    'svm__alpha':[1e-2, 1e-3]
}


#%%time
train_test(pipeline, parameters, 'SVM linear kernel Confusion Matrix', X_train, y_train, X_test, y_test)


# #############################################################################
# Define a pipeline combining a text feature extractor with a
# Multinomial Naive Bayes classifier
pipeline = Pipeline([
    ('vect', CountVectorizer(ngram_range= (1,1))),
    ('tfidf', TfidfTransformer()),
    ('nb', MultinomialNB(class_prior=None, fit_prior=False)),
])


parameters = {
    'vect__max_df': [0.5, 0.75, 1.0],
    'vect__tokenizer':[LemmaTokenizer(),StemmerTokenizer(), tokenizer],
    'vect__stop_words': [None, stops],
    'tfidf__use_idf': [True, False],
    'nb__alpha': [0.1,  0.5 , 1.0 ],
    
}

#%%time
train_test(pipeline, parameters, "Naive Bayes Confusion Matrix", X_train, y_train, X_test, y_test)

# #############################################################################
# Define a pipeline combining a text feature extractor with a Random Forest
# classifier
pipeline = Pipeline([     
    ('vect', CountVectorizer(ngram_range= (1,1))),
    ('tfidf', TfidfTransformer()),
    ('rfclassifier', RandomForestClassifier(n_estimators=1000,  
    max_depth=None,
    min_samples_split=10, min_samples_leaf=1,
    min_weight_fraction_leaf=0.0,
    max_features='log2',
    max_leaf_nodes=None, bootstrap=True, 
    oob_score=True, 
    n_jobs=4, 
    random_state=42, verbose=0,criterion='gini',
    warm_start=False, class_weight=None))
])

parameters = {
    'vect__max_df': [0.5, 0.75, 1.0],
    'vect__tokenizer':[LemmaTokenizer(),StemmerTokenizer(), tokenizer],
    'vect__stop_words': [None, stops],
    'tfidf__use_idf': [True, False],
    'rfclassifier__max_features': ['sqrt', 'log2'],
    'rfclassifier__criterion': ['gini', 'entropy']   
}

#%%time
train_test(pipeline, parameters, "Random Forest Confusion Matrix", X_train, y_train, X_test, y_test)

# #############################################################################
# Define a pipeline combining a text feature extractor with Random GuessBaseline
# classifier
pipeline = Pipeline([
    ('vect', CountVectorizer(ngram_range= (1,1))),
    ('tfidf', TfidfTransformer()),
    ('random', DummyClassifier(strategy="uniform", random_state=42))
])

parameters = {
    'vect__max_df': [0.5, 0.75, 1.0],
    'vect__tokenizer':[LemmaTokenizer(),StemmerTokenizer(), tokenizer],
    'vect__stop_words': [stops, None],
    'tfidf__use_idf': (True, False)   
}

#%%time
train_test(pipeline, parameters, "Random Baseline Confusion Matrix", X_train, y_train, X_test, y_test)
